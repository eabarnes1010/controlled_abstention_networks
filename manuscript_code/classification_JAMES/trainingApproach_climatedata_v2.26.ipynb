{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Abstention Loss\n",
    "author: Elizabeth A. Barnes, Randal J. Barnes\n",
    "date: January 15, 2021, 0738MST\n",
    "\n",
    "* based on Thulasidasan, S., T. Bhattacharya, J. Bilmes, G. Chennupati, and J. Mohd-Yusof, 2019: Combating Label Noise in Deep Learning Using Abstention. arXiv [stat.ML],.\n",
    "* thesis: https://digital.lib.washington.edu/researchworks/handle/1773/45781\n",
    "* code base is here: https://github.com/thulas/dac-label-noise/blob/master/dac_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import collections\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy as ct\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import abstentionloss\n",
    "import metrics\n",
    "import network\n",
    "import plots\n",
    "import climatedata\n",
    "import experiments\n",
    "\n",
    "import imp\n",
    "imp.reload(experiments)\n",
    "imp.reload(abstentionloss)\n",
    "imp.reload(plots)\n",
    "imp.reload(climatedata)\n",
    "\n",
    "import palettable\n",
    "import pprint\n",
    "\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "mpl.rcParams['figure.dpi']= 150\n",
    "dpiFig = 300.\n",
    "\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "tf.print(f\"sys.version = {sys.version}\", output_stream=sys.stdout)\n",
    "tf.print(f\"tf.version.VERSION = {tf.version.VERSION}\", output_stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------\n",
    "DATA_NAME = 'tranquilFOO23'#'tranquilFOO0'\n",
    "SCRIPT_NAME = 'trainingApproach_climatedata_v2.26_cmdA.py'\n",
    "checkpointDir = '/Users/eabarnes/Data/2021/abstention_loss/checkpoints/'\n",
    "EXPINFO = experiments.define_experiments(DATA_NAME)\n",
    "pprint.pprint(EXPINFO, width=60)\n",
    "#--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NP_SEED = 99\n",
    "np.random.seed(NP_SEED)\n",
    "tf.random.set_seed(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_ipynb():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if 'IPKernelApp' not in get_ipython().config:  # pragma: no cover\n",
    "            mpl.use('Agg')            \n",
    "            return False\n",
    "    except:\n",
    "        mpl.use('Agg')        \n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_name(loss, data_name, extra_text = ''):\n",
    "    # set experiment name\n",
    "    if loss == 'DNN':\n",
    "        EXP_NAME = (\n",
    "            data_name\n",
    "            + '_DNN'\n",
    "            + '_prNoise' + str(PR_NOISE)\n",
    "            + '_networkSeed' + str(NETWORK_SEED)\n",
    "            + '_npSeed' + str(NP_SEED)\n",
    "        )                \n",
    "    else:\n",
    "        EXP_NAME = (\n",
    "            data_name\n",
    "            + '_' + loss\n",
    "            + '_' + UPDATER\n",
    "            + '_abstSetpoint' + str(setpoint)\n",
    "            + '_prNoise' + str(PR_NOISE)\n",
    "            + '_networkSeed' + str(NETWORK_SEED)\n",
    "            + '_npSeed' + str(NP_SEED)\n",
    "    )\n",
    "\n",
    "    return EXP_NAME + extra_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(loss_str = 'DNN', updater_str='Colorado', setpoint=.5, spinup_epochs=10, nupd=10):\n",
    "    # Define and train the model\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    if(loss_str == 'DNN'):\n",
    "        model = network.defineNN(hiddens, input_shape=X_train_std.shape[1], output_shape=NLABEL, ridge_penalty=RIDGE, act_fun='relu', network_seed=NETWORK_SEED)\n",
    "        loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        model.compile(\n",
    "            optimizer=optimizers.SGD(lr=LR_INIT, momentum=0.9, nesterov=True),\n",
    "            loss = loss_function,\n",
    "            metrics=[\n",
    "                metrics.AbstentionFraction(NLABEL),\n",
    "                metrics.PredictionAccuracy(NLABEL)\n",
    "            ]\n",
    "        )        \n",
    "    else:\n",
    "        model = network.defineNN(hiddens, input_shape=X_train_std.shape[1], output_shape=NLABEL+1, ridge_penalty=RIDGE, act_fun='relu', network_seed=NETWORK_SEED)\n",
    "        updater = getattr(abstentionloss, updater_str)(setpoint=setpoint, \n",
    "                                                       alpha_init=.5, \n",
    "                                                       length=nupd)\n",
    "        loss_function = getattr(abstentionloss, loss_str)(updater=updater,\n",
    "                                                          spinup_epochs=spinup_epochs)\n",
    "        model.compile(\n",
    "            optimizer=optimizers.SGD(lr=LR_INIT, momentum=0.9, nesterov=True),\n",
    "            loss = loss_function,\n",
    "            metrics=[\n",
    "                alpha_value,\n",
    "                metrics.AbstentionFraction(NLABEL),\n",
    "                metrics.PredictionLoss(NLABEL),\n",
    "                metrics.PredictionAccuracy(NLABEL)\n",
    "            ]\n",
    "        )        \n",
    "        \n",
    "    # model.summary()\n",
    "\n",
    "        \n",
    "    return model, loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "if 'SSTrand' not in globals():\n",
    "    try:\n",
    "        SIMPLE_DATA = EXPINFO['simple_data']\n",
    "    except KeyError:\n",
    "        SIMPLE_DATA = False\n",
    "\n",
    "    try:\n",
    "        REGION_NAME = EXPINFO['foo_region']\n",
    "    except KeyError:\n",
    "        REGION_NAME = 'ENSO'\n",
    "        \n",
    "    if(SIMPLE_DATA==True):\n",
    "        SSTrand, y, lat, lon = climatedata.load_simpledata(size='15x60')\n",
    "    elif(SIMPLE_DATA==False):\n",
    "        SSTrand, y, lat, lon = climatedata.load_data()\n",
    "    else:\n",
    "        SSTrand, y, lat, lon = climatedata.load_simpledata(size=SIMPLE_DATA)\n",
    "\n",
    "lat = np.squeeze(lat)\n",
    "lon = np.squeeze(lon)\n",
    "print('SST shape = ' + str(np.shape(SSTrand)))\n",
    "\n",
    "# define the ENSO region\n",
    "reg_lats, reg_lons = climatedata.get_region(region_name = REGION_NAME)\n",
    "\n",
    "# plot the data\n",
    "cmap = palettable.cartocolors.diverging.Geyser_7.mpl_colormap\n",
    "    \n",
    "if in_ipynb():\n",
    "    plt.figure(figsize=(12,2.73*2))\n",
    "    mapProj = ct.crs.EqualEarth(central_longitude = 0.)\n",
    "    ax = plt.subplot(1,2,1,projection=mapProj)\n",
    "    cb, image = plots.drawOnGlobe(ax, \n",
    "                            mapProj,\n",
    "                            SSTrand[20,:,:], \n",
    "                            np.squeeze(lat), \n",
    "                            np.squeeze(lon), \n",
    "                            cmap = cmap, \n",
    "                            vmin = -3, \n",
    "                            vmax=3, \n",
    "                            cbarBool=True, \n",
    "                            fastBool=True, \n",
    "                            extent='both'\n",
    "                           )\n",
    "    plt.plot([reg_lons[0], reg_lons[0],reg_lons[1],reg_lons[1],reg_lons[0]], [reg_lats[0], reg_lats[1], reg_lats[1], reg_lats[0],reg_lats[0]],\n",
    "             color='white', linestyle='--',\n",
    "             transform=ccrs.PlateCarree(),\n",
    "             )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(climatedata)\n",
    "np.random.seed(NP_SEED)\n",
    "\n",
    "NLABEL = EXPINFO['numClasses']\n",
    "NSAMPLES = EXPINFO['nSamples']\n",
    "PR_NOISE = EXPINFO['prNoise']\n",
    "CUTOFF = EXPINFO['cutoff']\n",
    "UNDERSAMPLE = EXPINFO['undersample']\n",
    "\n",
    "#----------------------------\n",
    "X, y_cat, tranquil, corrupt, y_perc = climatedata.add_noise(data_name=DATA_NAME, \n",
    "                                                   X=SSTrand[:NSAMPLES], \n",
    "                                                   y=y[:NSAMPLES], \n",
    "                                                   lat=lat, \n",
    "                                                   lon=lon, \n",
    "                                                   pr_noise=PR_NOISE, \n",
    "                                                   nlabel=NLABEL, \n",
    "                                                   cutoff=CUTOFF,\n",
    "                                                   region_name=REGION_NAME,                                                            \n",
    "                                                  )\n",
    "data_train, data_val, data_test = climatedata.split_data(X, y_cat, tranquil, corrupt)\n",
    "X_train, y_train, tr_train, cr_train = data_train\n",
    "X_val, y_val, tr_val, cr_val = data_val\n",
    "\n",
    "print('Train Shape = ' + str(np.shape(X_train)))\n",
    "print('Validation Shape = ' + str(np.shape(X_val)))\n",
    "\n",
    "# undersample the data\n",
    "if UNDERSAMPLE:\n",
    "    print('----Training----')\n",
    "    X_train, y_train, tr_train = climatedata.undersample(X_train, y_train, tr_train) # training data\n",
    "    print('total samples = ' + str(np.shape(X_train)[0]))    \n",
    "    print('----Validation----')\n",
    "    X_val, y_val, tr_val = climatedata.undersample(X_val, y_val, tr_val) # validation data\n",
    "    print('total samples = ' + str(np.shape(X_val)[0]))               \n",
    "    \n",
    "# process data for training\n",
    "X_train_std, onehotlabels, X_val_std, onehotlabels_val, xmean, xstd = climatedata.preprocess_data(X_train, y_train, X_val, y_val, NLABEL)\n",
    "\n",
    "if in_ipynb():\n",
    "    plt.figure(figsize=(6*1.5,3*1.5))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.hist(y_train,np.arange(0,NLABEL+1))\n",
    "    plt.xlabel('labels')\n",
    "    plt.title('all')\n",
    "    \n",
    "    plt.subplot(2,2,4)\n",
    "    plt.hist(y_train[cr_train==1],np.arange(0,NLABEL+1))\n",
    "    plt.xlabel('class')\n",
    "    plt.title('corrupted labels')\n",
    "        \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.hist(y_train[tr_train==1],np.arange(0,NLABEL+1))\n",
    "    plt.xlabel('class')\n",
    "    plt.title('tranquil labels')\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.hist(y_train[tr_train==0],np.arange(0,NLABEL+1))\n",
    "    plt.xlabel('class')\n",
    "    plt.title('not tranquil')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_value(y_true,y_pred):\n",
    "    return loss_function.updater.alpha\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < lr_epoch_bound:\n",
    "        return lr\n",
    "    else:\n",
    "        return LR_INIT/2.#lr*tf.math.exp(-0.1)\n",
    "\n",
    "class EarlyStoppingDAC(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):\n",
    "        super(EarlyStoppingDAC, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as zero.\n",
    "        self.best = 0.\n",
    "        self.best_epoch = np.Inf\n",
    "        # initialize best_weights to non-trained model\n",
    "        self.best_weights = self.model.get_weights()\n",
    "        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"val_prediction_accuracy\")\n",
    "        if np.greater(current, self.best):\n",
    "            abstention_error = np.abs(logs.get(\"val_abstention_fraction\") - setpoint)\n",
    "            if np.less(abstention_error,.1):\n",
    "                self.best = current\n",
    "                self.wait = 0\n",
    "                # Record the best weights if current results is better (greater).\n",
    "                self.best_weights = self.model.get_weights()\n",
    "                self.best_epoch = epoch\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Early stopping, setting to best_epoch = \" + str(self.best_epoch + 1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS = EXPINFO['loss']\n",
    "UPDATER = EXPINFO['updater']\n",
    "REWRITE = False\n",
    "SAVE_HISTORY = True\n",
    "EXTRA_TEXT = ''\n",
    "#---------------------\n",
    "# Set parameters\n",
    "NUPD = EXPINFO['nupd']\n",
    "hiddens = EXPINFO['hiddens']\n",
    "SPINUP_EPOCHS = EXPINFO['spinup']\n",
    "BATCH_SIZE = EXPINFO['batch_size']\n",
    "LR_INIT = EXPINFO['lr_init']\n",
    "\n",
    "N_EPOCHS = 200\n",
    "lr_epoch_bound = 10000\n",
    "RIDGE = 0.\n",
    "#---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach_dic = {'DNN':'', \n",
    "                'DAC':'', \n",
    "#                 'DNN-DNN':'_postDNN-DNN', \n",
    "#                 'DAC-DNN':'_postDAC-DNN', \n",
    "                'ORACLE':'_oracle', \n",
    "#                 'SELENE':'_selene'\n",
    "               }\n",
    "abstain_setpoint = np.around(np.arange(0., 1., .1), 3)\n",
    "seed_vector = np.arange(0,50)\n",
    "\n",
    "if in_ipynb():\n",
    "    NETWORK_SEED_LIST = (0,)\n",
    "else:\n",
    "    NETWORK_SEED_LIST = (int(sys.argv[-1]),)\n",
    "    if(NETWORK_SEED_LIST[0]>np.max(seed_vector)):\n",
    "        sys.exit()\n",
    "for NETWORK_SEED in NETWORK_SEED_LIST:\n",
    "    for setpoint in abstain_setpoint:\n",
    "        for app in approach_dic.keys():\n",
    "\n",
    "            # skipping rules----\n",
    "            if(setpoint==0):\n",
    "                if((app != 'DNN') and (app != 'ORACLE') and (app != 'SELENE')):\n",
    "                    continue\n",
    "            else:\n",
    "                if((app=='DNN') or app=='ORACLE' or app=='SELENE'):\n",
    "                    continue\n",
    "            #-------------------\n",
    "\n",
    "            if((app=='DNN') or (app=='ORACLE' or app=='SELENE')):\n",
    "                EXP_NAME = get_exp_name(loss = 'DNN', data_name=DATA_NAME, extra_text=approach_dic[app])\n",
    "            elif(app=='DAC'):\n",
    "                EXP_NAME = get_exp_name(loss = LOSS, data_name=DATA_NAME, extra_text = approach_dic[app])\n",
    "            elif(app=='DNN-DNN' or app=='DAC-DNN'):\n",
    "                EXP_NAME = get_exp_name(loss = 'DNN', data_name=DATA_NAME, extra_text=approach_dic[app])\n",
    "                i = EXP_NAME.find('prNoise')\n",
    "                EXP_NAME = EXP_NAME[:i] + 'abstSetpoint' + str(setpoint) + '_' + EXP_NAME[i:]\n",
    "            else:\n",
    "                raise ValueError('no such approach')\n",
    "            model_name = 'saved_models/model_' +  EXP_NAME\n",
    "\n",
    "            if(os.path.exists((model_name + '.h5').format(N_EPOCHS)) and REWRITE==False):\n",
    "                continue\n",
    "            else:\n",
    "                print(EXP_NAME)   \n",
    "\n",
    "            #-------------------------------\n",
    "            # Determine indices to grab for training of the different approaches\n",
    "            if((app=='DNN') or (app=='DAC')):\n",
    "                i_train = np.arange(0,np.shape(onehotlabels)[0])\n",
    "                i_val = np.arange(0,np.shape(onehotlabels_val)[0])\n",
    "\n",
    "            elif(app=='ORACLE'):\n",
    "                i_train = np.where(cr_train==0)[0]\n",
    "                i_val = np.where(cr_val==0)[0]\n",
    "                \n",
    "            elif(app=='SELENE'):\n",
    "                i_train = np.where(tr_train==1)[0]\n",
    "                i_val = np.where(tr_val==1)[0]\n",
    "                \n",
    "            elif(app=='DNN-DNN'):\n",
    "                exp_name_0 = get_exp_name(loss = 'DNN', data_name=DATA_NAME, extra_text='')\n",
    "                model_name_0 = 'saved_models/model_' +  exp_name_0 + '.h5'\n",
    "                model0, __ = make_model(loss_str = 'DNN')\n",
    "                model0.load_weights(model_name_0)\n",
    "\n",
    "                y_pred_train_0 = model0.predict(X_train_std)\n",
    "                y_pred_val_0 = model0.predict(X_val_std)\n",
    "                max_logits = np.max(y_pred_train_0,axis=-1)\n",
    "                i_train = np.where(max_logits >= np.percentile(max_logits, 100*setpoint))[0]\n",
    "                max_logits = np.max(y_pred_val_0,axis=-1)\n",
    "                i_val = np.where(max_logits >= np.percentile(max_logits, 100*setpoint))[0]            \n",
    "\n",
    "            elif(app=='DAC-DNN'):\n",
    "                exp_name_0 = get_exp_name(loss = LOSS, data_name=DATA_NAME, extra_text='')\n",
    "                model_name_0 = 'saved_models/model_' +  exp_name_0 + '.h5'\n",
    "                model0, __ = make_model(loss_str = LOSS)\n",
    "                model0.load_weights(model_name_0)\n",
    "\n",
    "                y_pred_train_0 = model0.predict(X_train_std)\n",
    "                y_pred_val_0 = model0.predict(X_val_std)\n",
    "                i_train = np.where(np.argmax(y_pred_train_0,axis=-1) != NLABEL)[0]\n",
    "                i_val = np.where(np.argmax(y_pred_val_0,axis=-1) != NLABEL)[0]\n",
    "\n",
    "            else:\n",
    "                raise ValueError('no such app')\n",
    "\n",
    "            #-------------------------------\n",
    "            # Get the model\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            # callbacks\n",
    "            lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=0)\n",
    "            cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath = checkpointDir + 'model_' + EXP_NAME + '_epoch{epoch:03d}.h5', \n",
    "                verbose=0, \n",
    "                save_weights_only=True,\n",
    "            )\n",
    "\n",
    "            # define the model and loss function\n",
    "            if(app=='DAC'):\n",
    "                es_dac_callback = EarlyStoppingDAC(patience=30)            \n",
    "                model, loss_function = make_model(loss_str = LOSS, \n",
    "                                                  updater_str=UPDATER, \n",
    "                                                  setpoint=setpoint, \n",
    "                                                  spinup_epochs=SPINUP_EPOCHS,\n",
    "                                                  nupd=NUPD)\n",
    "                callbacks = [abstentionloss.AlphaUpdaterCallback(), lr_callback, cp_callback, es_dac_callback]            \n",
    "            else:\n",
    "                es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_prediction_accuracy', patience=30, mode='max', restore_best_weights=True, verbose=1)                    \n",
    "                model, loss_function = make_model(loss_str = 'DNN')\n",
    "                callbacks = [lr_callback, cp_callback, es_callback]\n",
    "\n",
    "            #-------------------------------\n",
    "            # Remake onehotencoding\n",
    "            hotlabels = onehotlabels[:,:model.output_shape[-1]] # strip off abstention class if using the DNN\n",
    "            hotlabels_val = onehotlabels_val[:,:model.output_shape[-1]] # strip off abstention class if using the DNN\n",
    "\n",
    "            #-------------------------------\n",
    "            # Train the model\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            try:\n",
    "                history = model.fit(\n",
    "                    X_train_std[i_train],\n",
    "                    hotlabels[i_train],\n",
    "                    validation_data=(X_val_std[i_val], hotlabels_val[i_val]),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=N_EPOCHS,\n",
    "                    shuffle=True,\n",
    "                    verbose=0,\n",
    "                    callbacks=callbacks\n",
    "                )\n",
    "                if(SAVE_HISTORY):\n",
    "                    # save history data\n",
    "                    history_dict = model.history.history\n",
    "                    history_file = 'saved_models/history_' +  EXP_NAME + '.pickle'\n",
    "                    with open(history_file, 'wb') as handle:\n",
    "                        pickle.dump(history_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "            stop_time = time.time()\n",
    "            tf.print(f\"Elapsed time during fit = {stop_time - start_time:.2f} seconds\\n\")\n",
    "            \n",
    "            model.save_weights(model_name + '.h5')\n",
    "            for f in glob.glob(checkpointDir + 'model_' + EXP_NAME + \"_epoch*.h5\"):\n",
    "                os.remove(f)        \n",
    "\n",
    "            #-------------------------------\n",
    "            # Display the results\n",
    "\n",
    "            exp_info=(LOSS, N_EPOCHS, setpoint, SPINUP_EPOCHS, hiddens, LR_INIT, lr_epoch_bound, BATCH_SIZE, NETWORK_SEED)\n",
    "\n",
    "            plots.plot_results(\n",
    "                EXP_NAME,\n",
    "                history,\n",
    "                exp_info=exp_info,\n",
    "                saveplot=True,\n",
    "                showplot=True\n",
    "            )\n",
    "\n",
    "if in_ipynb()==False:    \n",
    "    print('-----starting new kernel-----')                \n",
    "    os.execv(sys.executable, ['python'] + ['/Users/eabarnes/GoogleDrive/WORK/RESEARCH/2021/abstention_networks/' + SCRIPT_NAME] + [str(NETWORK_SEED+1)])        \n",
    "    print('-----exiting...')        \n",
    "    sys.exit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_val_std[i_val], hotlabels_val[i_val])\n",
    "# model.evaluate(x=X_val_std[i_val], y=hotlabels_val[i_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
